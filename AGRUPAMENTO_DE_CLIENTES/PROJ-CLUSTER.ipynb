{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infraestrutura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importando as libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de pacotes a serem instalados\n",
    "packages = [\n",
    "    \"matplotlib\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"scipy\",\n",
    "    \"seaborn\",\n",
    "    \"scikit-learn\",\n",
    "    \"pyautogui\",\n",
    "    \"pillow\",\n",
    "    \"kmodes\"\n",
    "]\n",
    "\n",
    "# Função para instalar pacotes com tentativa de pip e conda\n",
    "def install_package(package):\n",
    "    try:\n",
    "        # Tentar instalar com pip\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"{package} instalado com sucesso via pip!\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Erro ao instalar o pacote {package} via pip. Tentando alternativas...\")\n",
    "        if package == \"scikit-learn-extra\":\n",
    "            # Tentativa de instalação via conda para scikit-learn-extra\n",
    "            try:\n",
    "                print(\"Tentando instalar scikit-learn-extra usando conda...\")\n",
    "                subprocess.check_call([\"conda\", \"install\", \"-c\", \"conda-forge\", \"scikit-learn-extra\", \"-y\"])\n",
    "                print(\"scikit-learn-extra instalado com sucesso via conda!\")\n",
    "            except subprocess.CalledProcessError:\n",
    "                print(f\"Erro ao instalar {package} com conda. Verifique seu ambiente Conda.\")\n",
    "        else:\n",
    "            print(f\"Não há alternativa para o pacote {package}. Instale manualmente se necessário.\")\n",
    "\n",
    "# Atualizar pip, setuptools e wheel\n",
    "try:\n",
    "    print(\"Atualizando pip, setuptools e wheel...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"])\n",
    "    print(\"Atualização de pip, setuptools e wheel concluída com sucesso!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Erro ao atualizar pip, setuptools e wheel. Detalhes: {e}\")\n",
    "\n",
    "# Verificar e instalar pacotes\n",
    "for package in packages:\n",
    "    try:\n",
    "        # Tentar importar o pacote para verificar se já está instalado\n",
    "        __import__(package.split(\"-\")[0])  # Divide em casos de pacotes com nomes compostos\n",
    "        print(f\"{package} já está instalado.\")\n",
    "    except ImportError:\n",
    "        print(f\"{package} não está instalado. Instalando...\")\n",
    "        install_package(package)\n",
    "\n",
    "# Mensagem final\n",
    "print(\"\\nVerificação de pacotes concluída.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from functools import reduce\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ydata_profiling import ProfileReport\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pyautogui\n",
    "import math\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.stats import norm\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você está rodando em Python 3.9+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Versão do Python:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você está usando um ambiente virtual: Virtualenv ou Anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ambiente virtual ativo:\", sys.prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas as bibliotecas usadas nesse exercícios estão instaladas em um ambiente virtual específico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gere um arquivo de requerimentos (requirements.txt) com os pacotes necessários. É necessário se certificar que a versão do pacote está disponibilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera o arquivo requirements.txt com os pacotes instalados\n",
    "with open(\"requirements.txt\", \"w\") as file:\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"freeze\"], stdout=file)\n",
    "\n",
    "print(\"Arquivo requirements.txt gerado com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tire um printscreen do ambiente que será usado rodando em sua máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tira o print da tela e salva no arquivo screenshot.png\n",
    "screenshot = pyautogui.screenshot()\n",
    "screenshot.save(\"screenshot.png\")\n",
    "\n",
    "print(\"Screenshot salva como 'screenshot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disponibilize os códigos gerados, assim como os artefatos acessórios (requirements.txt) e instruções em um repositório GIT público. (se isso não for feito, o diretório com esses arquivos deverá ser enviado compactado no moodle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('link para o GITHUB: https://github.com/CBarrosoBRRJ/Algoritmos_de_Inteligencia_Artificial_para_Clusterizacao')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignorar avisos\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações de exibição do Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Garantir que seaborn e matplotlib tenham gráficos amigáveis\n",
    "# Configuração para visualizações\n",
    "plt.style.use(\"default\")\n",
    "sns.set_theme(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de base de dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolha uma base de dados para realizar o trabalho. Essa base será usada em um problema de clusterização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DATA\\DF_CLUSTER.csv', encoding='latin-1', on_bad_lines='skip', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo informações gerais do dataset\n",
    "print(\"\\nInformações do dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escreva a justificativa para a escolha de dados, dando sua motivação e objetivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Essa base é uma base real do meu trabalho, e a motivação é tentar aplicar o método em uma base que pode virar um projeto profissional.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostre através de gráficos a faixa dinâmica das variáveis que serão usadas nas tarefas de clusterização. Analise os resultados mostrados. O que deve ser feito com os dados antes da etapa de clusterização?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolher as cores do heatmap (exemplo de gradiente personalizado)\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"#003f5c\", \"#FFF\", \"#ffa600\"])\n",
    "\n",
    "# Filtrar colunas categóricas relevantes\n",
    "categorical_cols = [col for col in df.select_dtypes(include=['object', 'category']).columns]\n",
    "\n",
    "# Garantir que existam colunas categóricas no DataFrame\n",
    "if not categorical_cols:\n",
    "    print(\"Não há colunas categóricas no DataFrame para análise.\")\n",
    "else:\n",
    "    # Criar um DataFrame com contagens absolutas para as variáveis binárias\n",
    "    count_data = df[categorical_cols].apply(lambda x: x.value_counts()).T.fillna(0)\n",
    "\n",
    "    # Formatar os números absolutos com pontos de milhar\n",
    "    count_data_formatted = count_data.applymap(lambda x: f\"{int(x):,}\".replace(\",\", \".\"))\n",
    "\n",
    "    # Criar o DataFrame com percentuais\n",
    "    percent_data = (count_data.div(count_data.sum(axis=1), axis=0) * 100).round(1)\n",
    "\n",
    "    # Unificar contagens e percentuais\n",
    "    heatmap_data = count_data_formatted + \" (\" + percent_data.astype(str) + \"%)\"\n",
    "\n",
    "    # Criar o heatmap com valores absolutos e percentuais\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(\n",
    "        count_data,  # O heatmap utiliza os dados numéricos para as cores\n",
    "        annot=heatmap_data.values,  # Adiciona a contagem e percentual como texto\n",
    "        fmt=\"\",  # Evita formatação padrão de números\n",
    "        cmap=custom_cmap,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"label\": \"Contagem\"}  # Adiciona uma barra de cores\n",
    "    )\n",
    "    plt.title(\"Heatmap de Contagens e Percentuais de Variáveis Categóricas\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Categorias\", fontsize=14)\n",
    "    plt.ylabel(\"Variáveis\", fontsize=14)\n",
    "    plt.xticks(fontsize=12, rotation=0)\n",
    "    plt.yticks(fontsize=15, rotation=0, font='arial')\n",
    "    plt.show()\n",
    "\n",
    "    # Explicação sobre como analisar\n",
    "    print(\n",
    "        \"\\nAnálise do Heatmap:\\n\"\n",
    "        \"1. As linhas representam as variáveis categóricas do dataset.\\n\"\n",
    "        \"2. As colunas mostram as categorias dentro de cada variável (ex.: 'Sim' e 'Não').\\n\"\n",
    "        \"3. Os valores no heatmap mostram a contagem absoluta (com pontos de milhar), seguidos do percentual em parênteses.\\n\"\n",
    "        \"4. Utilize este gráfico para identificar variáveis desbalanceadas ou categorias dominantes.\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realize o pré-processamento adequado dos dados. Descreva os passos necessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"1. Entendimento do problema e dos dados\\n\"\n",
    "    \"2. Pré-processamento dos dados\\n\"\n",
    "    \"3. Redução de dimensionalidade (ex.: PCA)\\n\"\n",
    "    \"4. Definição de métricas de qualidade\\n\"\n",
    "    \"5. Escolha do número de clusters\\n\"\n",
    "    \"6. Aplicação de algoritmos de clusterização\\n\"\n",
    "    \"7. Avaliação e comparação de modelos\\n\"\n",
    "    \"8. Visualização dos clusters\\n\"\n",
    "    \"9. Interpretação e aplicação dos resultados\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando valores binários ('S', 'N') para numéricos (1, 0)\n",
    "df_numeric = pd.get_dummies(df)\n",
    "df_numeric.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando as distancias entre os dados\n",
    "X = df_numeric.div(df_numeric.sum(axis=1), axis='rows')\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste no PCA para suportar proporção de variância explicada\n",
    "n_componentes = 0.9  # Alvo de 90% da variância explicada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redução de dimensionalidade com PCA\n",
    "pca = PCA(n_components=n_componentes)\n",
    "pca_result = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinar o número de componentes para 90% da variância explicada\n",
    "explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "components_90_variance = next((i + 1 for i, cumulative_variance in enumerate(explained_variance) if cumulative_variance >= n_componentes), None)\n",
    "print(f\"Componentes necessários para 90% da variância: {components_90_variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if components_90_variance is None:\n",
    "    print(\"Nenhum número de componentes atinge 90% da variância explicada.\")\n",
    "    print(f\"A maior variância explicada acumulada é {explained_variance[-1]:.2%}.\")\n",
    "else:\n",
    "    print(f\"O número de componentes principais necessários para explicar pelo menos 90% da variância é {components_90_variance}, que acumula aproximadamente {explained_variance[components_90_variance - 1]:.2%} da variância.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar variância explicada acumulada\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--', label='Cumulative Explained Variance')\n",
    "plt.axhline(y=n_componentes, color='r', linestyle='--', label='90% Variance Threshold') # Rotulo\n",
    "\n",
    "if components_90_variance is not None:\n",
    "    plt.axvline(x=components_90_variance, color='g', linestyle='--', label=f'{components_90_variance} Components')\n",
    "\n",
    "plt.title('Cumulative Explained Variance by PCA Components')\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a variância explicada pelos dois componentes principais\n",
    "pca = PCA(n_components=2)\n",
    "pca_2d = pca.fit_transform(X)\n",
    "explained_variance_2d = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "# Visualizando PCA com 2 Componentes para interpretação gráfica\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], alpha=0.6, s=10, color='purple')\n",
    "plt.title(\"PCA - Visualização com 2 Componentes Principais\")\n",
    "plt.xlabel(\"Componente 1\")\n",
    "plt.ylabel(\"Componente 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Informando o percentual de variância explicada\n",
    "print(f\"Os dois primeiros componentes principais explicam {explained_variance_2d:.2%} da variância total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição de métricas de qualidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escolha do número de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.utils import resample  # Para amostragem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimização por amostragem\n",
    "sample_size = 10000  # Defina o tamanho da amostra\n",
    "pca_sample = resample(X, n_samples=min(sample_size, len(pca_result)), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar listas\n",
    "num_clusters = list(range(2, 20))  # Ajuste o intervalo conforme necessário\n",
    "soma_quadrados_entre_cluster = []\n",
    "silhouette_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soma total dos quadrados\n",
    "soma_total = sum(pdist(pca_sample) ** 2) / pca_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando análise do método do cotovelo com otimizações...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a primeira e segunda derivada\n",
    "inertia = soma_quadrados_entre_cluster  # Lista já existente no script\n",
    "first_derivative = np.diff(inertia)\n",
    "second_derivative = np.diff(first_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in num_clusters:\n",
    "    try:\n",
    "        # Criar o modelo KMeans\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, init='k-means++')\n",
    "        labels = kmeans.fit_predict(pca_sample)\n",
    "\n",
    "        # Soma dos quadrados intra-cluster\n",
    "        intra_cluster_sum = kmeans.inertia_\n",
    "        soma_quadrados_entre_cluster.append(soma_total - intra_cluster_sum)\n",
    "\n",
    "        # Calcular o Silhouette Score\n",
    "        if k > 1:  # Apenas para k > 1\n",
    "            sil_score = silhouette_score(pca_sample, labels, metric='euclidean')\n",
    "            silhouette_scores.append(sil_score)\n",
    "\n",
    "        # Feedback\n",
    "        print(f\"Clusters: {k}, Inércia: {intra_cluster_sum:.2f}, Silhueta: {sil_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao calcular para k={k}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar o índice do cotovelo\n",
    "elbow_index = np.argmax(second_derivative) + 2  # Adicionar 2 para alinhar ao número de clusters\n",
    "print(f\"Melhor número de clusters pelo Método do Cotovelo: {elbow_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar preenchimento das listas\n",
    "if not silhouette_scores or not soma_quadrados_entre_cluster:\n",
    "    print(\"Erro: As listas de Silhouette Scores ou Soma dos Quadrados Entre Clusters estão vazias.\")\n",
    "else:\n",
    "    # Determinar o melhor número de clusters com base no Silhouette Score\n",
    "    best_silhouette_index = silhouette_scores.index(max(silhouette_scores))\n",
    "    best_silhouette_cluster = num_clusters[best_silhouette_index]\n",
    "\n",
    "    # Determinar o melhor número de clusters com base no Método do Cotovelo\n",
    "    if len(soma_quadrados_entre_cluster) > 1:\n",
    "        elbow_index = np.argmin(np.diff(soma_quadrados_entre_cluster))\n",
    "        best_elbow_cluster = num_clusters[elbow_index + 1]\n",
    "    else:\n",
    "        best_elbow_cluster = None\n",
    "\n",
    "    # Exibir os resultados\n",
    "    print(\"\\nResultados Finais:\")\n",
    "    print(f\"Melhor número de clusters com base no Silhouette Score: {best_silhouette_cluster}\")\n",
    "    print(f\"Melhor Silhouette Score: {max(silhouette_scores):.4f}\")\n",
    "    if best_elbow_cluster:\n",
    "        print(f\"Melhor número de clusters com base no Método do Cotovelo: {best_elbow_cluster}\")\n",
    "    else:\n",
    "        print(\"Não foi possível determinar o melhor número de clusters pelo Método do Cotovelo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCálculos concluídos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os dois gráficos lado a lado\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Gráfico 1: Variância Explicada\n",
    "ax1.plot(num_clusters, soma_quadrados_entre_cluster[:len(num_clusters)] / soma_total * 100, 'b*-')\n",
    "ax1.set_ylim((0, 100))\n",
    "ax1.grid(True)\n",
    "ax1.axhline(y=70, color='r', linestyle='--', label='70% Variância Explicada')\n",
    "ax1.axhline(y=80, color='g', linestyle='--', label='80% Variância Explicada')\n",
    "if best_elbow_cluster:\n",
    "    ax1.axvline(x=best_elbow_cluster, color='purple', linestyle='--', label=f'Melhor k (Elbow) = {best_elbow_cluster}')\n",
    "    ax1.set_xlabel('Número de Clusters')\n",
    "    ax1.set_ylabel('Percentual de Variância Explicada')\n",
    "    ax1.set_title('Variância Explicada x Valor de K')\n",
    "    ax1.legend(loc='best')\n",
    "\n",
    "# Gráfico 2: Silhouette Score\n",
    "sns.lineplot(x=num_clusters, y=silhouette_scores[:len(num_clusters)], color='darkorange', marker='o', ax=ax2)\n",
    "ax2.axhline(y=0.50, color='r', linestyle='--', label='Limite Inferior Aceitável (0.5)')\n",
    "ax2.axhline(y=0.70, color='g', linestyle='--', label='Limite Superior Aceitável (0.7)')\n",
    "ax2.axvline(x=best_silhouette_cluster, color='purple', linestyle='--', label=f'Melhor Cluster (Silhouette) = {best_silhouette_cluster}')\n",
    "ax2.set_xlabel('Número de Clusters', size=12)\n",
    "ax2.set_ylabel('Silhouette Score', size=12)\n",
    "ax2.set_title('Silhouette Score por Número de Clusters', size=20)\n",
    "ax2.legend(loc='best')\n",
    "\n",
    "# Exibir os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinar o melhor número de clusters com base no Silhouette Score\n",
    "best_k_silhouette = num_clusters[np.argmax(silhouette_scores)]\n",
    "silhouette_silhouette = max(silhouette_scores)\n",
    "\n",
    "# Determinar o melhor número de clusters com base no cotovelo (se aplicável)\n",
    "if len(soma_quadrados_entre_cluster) > 1:\n",
    "    elbow_index = np.argmin(np.diff(soma_quadrados_entre_cluster))\n",
    "    best_num_clusters = num_clusters[elbow_index + 1] if elbow_index + 1 < len(num_clusters) else num_clusters[elbow_index]\n",
    "    silhouette_elbow = silhouette_scores[num_clusters.index(best_num_clusters)]\n",
    "else:\n",
    "    best_num_clusters = None\n",
    "    silhouette_elbow = None\n",
    "\n",
    "# Exibir métricas para o melhor k (cotovelo), se aplicável\n",
    "if silhouette_elbow is not None:\n",
    "    print(f\"\\nMétricas para o melhor k (cotovelo): {best_num_clusters}\")\n",
    "    print(f\" - Silhueta: {silhouette_elbow:.4f}\")\n",
    "\n",
    "# Exibir métricas para o melhor k com base na silhueta\n",
    "print(f\"\\nMétricas para o melhor k (silhueta): {best_k_silhouette}\")\n",
    "print(f\" - Silhueta: {silhouette_silhouette:.4f}\")\n",
    "\n",
    "# Avaliação da qualidade do cluster com base no Silhouette Score\n",
    "if silhouette_silhouette >= 0.50:\n",
    "    print(\"A qualidade do cluster para o melhor k (silhueta) é BOA.\")\n",
    "elif 0.25 <= silhouette_silhouette < 0.50:\n",
    "    print(\"A qualidade do cluster para o melhor k (silhueta) é MODERADA.\")\n",
    "else:\n",
    "    print(\"A qualidade do cluster para o melhor k (silhueta) é RUIM.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicação de algoritmos de clusterização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de clusters escolhidos para o exemplo\n",
    "n_clusters = best_k_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando KMeans nos dados reduzidos pelo PCA\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, init='k-means++')\n",
    "\n",
    "# Ajustando o KMeans com os dados transformados pelo PCA\n",
    "y_pred_kmeans = kmeans.fit_predict(X)  # Gerando os rótulos dos clusters\n",
    "\n",
    "# Agora você pode acessar os labels dos clusters\n",
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "kmeans_silhouette = silhouette_score(X, kmeans_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Simulando dados para magnitude e cardinalidade\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "magnitudes = np.linalg.norm(cluster_centers, axis=1)  # Magnitude dos centróides\n",
    "cardinalities = np.bincount(y_pred_kmeans)  # Cardinalidade de cada cluster\n",
    "\n",
    "# Regressão linear para a linha de evolução\n",
    "slope, intercept, r_value, p_value, std_err = linregress(cardinalities, magnitudes)\n",
    "trend_line = slope * np.array(cardinalities) + intercept\n",
    "\n",
    "# Criando uma figura com três gráficos lado a lado\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Gráfico de Magnitude\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(range(len(magnitudes)), magnitudes)\n",
    "plt.title('Gráfico de Magnitude')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xticks(range(len(magnitudes)))\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Gráfico de Cardinalidade\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(range(len(cardinalities)), cardinalities)\n",
    "plt.title('Gráfico de Cardinalidade')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Cardinalidade')\n",
    "plt.xticks(range(len(cardinalities)))\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Gráfico de Magnitude vs Cardinalidade\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(cardinalities, magnitudes, c=range(len(cardinalities)), cmap='viridis', s=100, label='Clusters')\n",
    "plt.plot(cardinalities, trend_line, color='red', linestyle='--', label='Linha de Evolução')\n",
    "plt.title('Magnitude vs Cardinalidade')\n",
    "plt.xlabel('Cardinalidade')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "for i in range(len(cardinalities)):\n",
    "    plt.text(cardinalities[i], magnitudes[i], f'Cluster {i}', fontsize=9)\n",
    "\n",
    "# Ajustando layout para que todos os gráficos estejam alinhados\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar o tamanho da figura para exibir os três gráficos lado a lado\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Gráfico 1: Visualizando os clusters e os centróides no plano 2D usando PCA\n",
    "ax1 = fig.add_subplot(131)\n",
    "sc1 = ax1.scatter(pca_sample[:, 0], pca_sample[:, 1], c=kmeans.labels_, cmap='viridis', alpha=0.6)  # Colorir pelos clusters\n",
    "ax1.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=200, marker='X', label='Centroides')  # Adicionar os centróides\n",
    "ax1.set_title(\"Clusters com base no PCA (2D) - Com Centróides\")\n",
    "ax1.set_xlabel(\"Componente Principal 1\")\n",
    "ax1.set_ylabel(\"Componente Principal 2\")\n",
    "fig.colorbar(sc1, ax=ax1, label=\"Clusters\")\n",
    "ax1.legend()\n",
    "\n",
    "# Gráfico 2: Refazendo o PCA com 3 componentes para visualização 3D\n",
    "pca_3d = PCA(n_components=3)\n",
    "pca_3d_data = pca_3d.fit_transform(pca_sample)  # Usar os dados amostrados\n",
    "\n",
    "# Gerando o gráfico 3D\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "sc2 = ax2.scatter(pca_3d_data[:, 0], pca_3d_data[:, 1], pca_3d_data[:, 2], c=kmeans.labels_, cmap='viridis', alpha=0.6)  # Colorir pelos clusters\n",
    "ax2.set_xlabel('Componente Principal 1')\n",
    "ax2.set_ylabel('Componente Principal 2')\n",
    "ax2.set_zlabel('Componente Principal 3')\n",
    "ax2.set_title('Visualização 3D dos Clusters')\n",
    "fig.colorbar(sc2, ax=ax2, label=\"Clusters\")\n",
    "\n",
    "# Gráfico 3: Silhouette\n",
    "ax3 = fig.add_subplot(133)\n",
    "y_lower = 10\n",
    "sample_silhouette_values = silhouette_samples(pca_sample, kmeans.labels_)\n",
    "\n",
    "# Configurar limites do gráfico de Silhouette\n",
    "ax3.set_xlim([-0.1, 1])\n",
    "ax3.set_ylim([0, len(pca_sample) + (kmeans.n_clusters + 1) * 10])\n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    # Valores de silhueta para o cluster i\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[kmeans.labels_ == i]\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / kmeans.n_clusters)\n",
    "    ax3.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label dos clusters no meio\n",
    "    ax3.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    y_lower = y_upper + 10  # Espaço entre clusters\n",
    "\n",
    "# Linha vertical do valor médio do Silhouette Score\n",
    "silhouette_avg = sample_silhouette_values.mean()\n",
    "ax3.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", label=f\"Média: {silhouette_avg:.2f}\")\n",
    "\n",
    "ax3.set_title(\"Gráfico de Silhouette para os Clusters\")\n",
    "ax3.set_xlabel(\"Valores do coeficiente de silhouette\")\n",
    "ax3.legend()\n",
    "\n",
    "# Ajustar layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Em construção')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
