{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando as Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from functools import reduce\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ydata_profiling import ProfileReport\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from scipy.stats import norm\n",
    "import statistics as st\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignorar avisos\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações de exibição do Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERIFICANDO GPUs DISPONÍVEIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código verifica quantas GPUs estão disponíveis para o TensorFlow. Isso é importante para determinar o hardware que será usado durante o treinamento do modelo, já que GPUs podem acelerar significativamente o processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coleta de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DATA\\DF_CLUSTER.csv', encoding='latin-1', on_bad_lines='skip', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploração e Análise dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui o objetivo desta análise é ter uma visão geral do conjunto de dados, compreendendo seu tamanho, variaveis, tipo de dados, e período dos dados coletados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tamanho do conjunto de dados. X Linhas, e Y Colunas\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visão geral das 5 primeiras linhas do conjunto de dados\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visão geral das 5 ultimas linhas do conjunto de dados\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizando algumas medidas estatisticas\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Informações básicas sobre tipos de variáveis\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total de valores únicos de cada variável\n",
    "#Se uma variável só tiver 1 valor, é sinal que não irá impactar no modelo e pode retira-lo do treinamento\n",
    "lista_vlr_unicos = []\n",
    "\n",
    "\n",
    "for i in df.columns[0:11].tolist():\n",
    "\tprint(i, \" : \", len(df[i].astype(str).value_counts()))\n",
    "\tlista_vlr_unicos.append(len(df[i].astype(str).value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APLICANDO DUMMYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORMALIZAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_binary(col):\n",
    "    # Verificar se a coluna contém apenas 0 e 1\n",
    "    unique_values = col.unique()\n",
    "    return set(unique_values).issubset({0, 1})\n",
    "\n",
    "# Verificar todas as colunas do DataFrame\n",
    "#binary_columns = [col for col in df.columns if is_binary(df[col])]\n",
    "\n",
    "# Exibir colunas binárias\n",
    "#print(f\"Colunas binárias: {binary_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas binárias\n",
    "#colunas_binarias = [col for col in df.columns if is_binary(df[col])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas não binárias\n",
    "#colunas_nao_binarias = df.columns.difference(binary_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir colunas binárias\n",
    "#print(f\"Colunas não binárias: {colunas_nao_binarias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a normalização apenas nas colunas não binárias\n",
    "scaler = MinMaxScaler()\n",
    "#try:\n",
    "  #  df[colunas_nao_binarias] = scaler.fit_transform(df[colunas_nao_binarias])\n",
    "#except:\n",
    "   # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APLICANDO O PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encontrar_melhor_pca(df, max_components):\n",
    "    # Garantir que o número máximo de componentes não exceda o limite permitido\n",
    "    max_components = min(max_components, df.shape[0], df.shape[1])\n",
    "    \n",
    "    melhor_inercia = float('inf')  # Melhor inércia (soma dos quadrados intra-cluster)\n",
    "    melhor_pca = None  # Melhor número de componentes\n",
    "    resultados = {\n",
    "        'n_componentes': [],\n",
    "        'inercia': [],\n",
    "        'var_exp_acumulada': [],\n",
    "        'silhouette_score': []\n",
    "    }\n",
    "\n",
    "    for n in range(1, max_components + 1):\n",
    "        pca = PCA(n_components=n)\n",
    "        df_pca = pca.fit_transform(df)  # Transformação PCA\n",
    "\n",
    "        # Aplicar KMeans após o PCA\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "        kmeans.fit(df_pca)\n",
    "\n",
    "        # Medir a inércia (quanto menor, melhor)\n",
    "        inercia = kmeans.inertia_\n",
    "\n",
    "        # Calcular a variância explicada acumulada\n",
    "        var_exp_acumulada = np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "        # Calcular o silhouette score\n",
    "        silhouette_avg = silhouette_score(df_pca, kmeans.labels_)\n",
    "\n",
    "        # Armazenar os resultados\n",
    "        resultados['n_componentes'].append(n)\n",
    "        resultados['inercia'].append(inercia)\n",
    "        resultados['var_exp_acumulada'].append(var_exp_acumulada)\n",
    "        resultados['silhouette_score'].append(silhouette_avg)\n",
    "\n",
    "        # Atualizar o melhor PCA com base na menor inércia\n",
    "        if inercia < melhor_inercia:\n",
    "            melhor_inercia = inercia\n",
    "            melhor_pca = n\n",
    "\n",
    "    print(f\"Melhor número de componentes PCA: {melhor_pca}, com inércia: {melhor_inercia:.4f}\")\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "# Atualizar o número de componentes com base no seu dataset\n",
    "resultados_pca = encontrar_melhor_pca(df, max_components=df.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ajustar os gráficos conforme os resultados\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Gráfico 1: Variância Explicada Acumulada\n",
    "ax1.plot(resultados_pca['n_componentes'], resultados_pca['var_exp_acumulada'], marker='o', linestyle='--', color='b', label='Variância Explicada Acumulada')\n",
    "ax1.axhline(y=0.70, color='r', linestyle='--', label='70% Variância Explicada')\n",
    "ax1.axhline(y=0.80, color='g', linestyle='--', label='80% Variância Explicada')\n",
    "ax1.set_xlabel('Número de Componentes')\n",
    "ax1.set_ylabel('Variância Explicada Acumulada')\n",
    "ax1.set_title('Variância Explicada Acumulada')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Gráfico 2: Silhouette Score\n",
    "ax2.plot(resultados_pca['n_componentes'], resultados_pca['silhouette_score'], marker='s', linestyle='-', color='orange', label='Silhouette Score')\n",
    "ax2.axhline(y=0.50, color='r', linestyle='--', label='Limite Inferior Aceitável (0.5)')\n",
    "ax2.axhline(y=0.70, color='g', linestyle='--', label='Limite Superior Aceitável (0.7)')\n",
    "ax2.set_xlabel('Número de Componentes')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score por Número de Componentes')\n",
    "ax2.legend(loc='best')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APLICANDO O PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_componentes = PCA(n_components=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pca_componentes.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir a quantidade de variância explicada pelos componentes\n",
    "explained_variance_ratio = pca_componentes.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinar a quantidade de componentes que explica 95% da variância\n",
    "variancia_acumulada = np.cumsum(explained_variance_ratio)  # Variância explicada acumulada\n",
    "melhor_n_componentes = np.argmax(variancia_acumulada >= 0.95) + 1  # Número mínimo de componentes que explicam 95% da variância"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTADOS - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando a contribuição de cada feature nos componentes principais\n",
    "fig, ax = plt.subplots(figsize=(20, 8))  # Aumentando a largura para 15 e altura para 6\n",
    "plt.imshow(\n",
    "    pca_componentes.components_.T,  # Transposta para exibir as features nas colunas\n",
    "    cmap=\"Spectral\",\n",
    "    vmin=-1,\n",
    "    vmax=1\n",
    ")\n",
    "plt.yticks(range(len(df.columns)), df.columns)\n",
    "plt.xticks(range(pca_componentes.n_components_), [f'PC{i+1}' for i in range(pca_componentes.n_components_)])\n",
    "plt.xlabel('Principais Componentes')\n",
    "plt.ylabel('Contribuição')\n",
    "plt.title('Contribuição das Features por Componentes')\n",
    "plt.colorbar()\n",
    "\n",
    "# Mostrando a figura\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ajustar os gráficos conforme os resultados\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Gráfico 1: Variância Explicada Acumulada\n",
    "ax1.plot(resultados_pca['n_componentes'], resultados_pca['var_exp_acumulada'], marker='o', linestyle='--', color='b', label='Variância Explicada Acumulada')\n",
    "ax1.axhline(y=0.70, color='r', linestyle='--', label='70% Variância Explicada')\n",
    "ax1.axhline(y=0.80, color='g', linestyle='--', label='80% Variância Explicada')\n",
    "ax1.set_xlabel('Número de Componentes')\n",
    "ax1.set_ylabel('Variância Explicada Acumulada')\n",
    "ax1.set_title('Variância Explicada Acumulada')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Gráfico 2: Silhouette Score\n",
    "ax2.plot(resultados_pca['n_componentes'], resultados_pca['silhouette_score'], marker='s', linestyle='-', color='orange', label='Silhouette Score')\n",
    "ax2.axhline(y=0.50, color='r', linestyle='--', label='Limite Inferior Aceitável (0.5)')\n",
    "ax2.axhline(y=0.70, color='g', linestyle='--', label='Limite Superior Aceitável (0.7)')\n",
    "ax2.set_xlabel('Número de Componentes')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score por Número de Componentes')\n",
    "ax2.legend(loc='best')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir a quantidade de variância explicada pelos componentes\n",
    "print(f'Variância explicada pelos componentes principais: {pca_componentes.explained_variance_ratio_}')\n",
    "print(f'Número de componentes selecionados: {pca_componentes.n_components_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicação dinâmica do que o PCA fez no DataFrame\n",
    "print(\"\\nO PCA (Principal Component Analysis) é uma técnica de redução de dimensionalidade.\")\n",
    "print(f\"Neste caso, o PCA reduziu o número de dimensões de {df.shape[1]} (colunas originais) para {pca_componentes.n_components_} componentes principais.\")\n",
    "print(\"Esses componentes principais são combinações lineares das variáveis originais, formadas para maximizar a variância explicada nos dados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para formatar a explicação das colunas principais em cada componente\n",
    "colunas_frequentes = set()  # Para armazenar as colunas que aparecem em todos os componentes\n",
    "for i, component in enumerate(pca_componentes.components_):\n",
    "    abs_component = abs(component)\n",
    "    sorted_indices = abs_component.argsort()[::-1]  # Ordena pelas contribuições absolutas em ordem decrescente\n",
    "    top_features = df.columns[sorted_indices[:3]]  # Pegando as 3 principais features\n",
    "    print(f\"As principais colunas para o Componente {i+1} são: {', '.join(top_features)}\")\n",
    "    print(f\"Este componente explica {explained_variance_ratio[i]*100:.2f}% da variância.\")\n",
    "    \n",
    "    # Armazenando as colunas frequentes\n",
    "    if i == 0:\n",
    "        colunas_frequentes = set(top_features)  # Inicia com as colunas do primeiro componente\n",
    "    else:\n",
    "        colunas_frequentes.intersection_update(set(top_features))  # Mantém apenas as colunas presentes em todos os componentes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variância explicada\n",
    "variancia_total = sum(pca_componentes.explained_variance_ratio_) * 100\n",
    "print(f\"A redução dimensional resultou em uma retenção de {variancia_total:.2f}% da variância total dos dados.\")\n",
    "print(f\"Isto significa que {100 - variancia_total:.2f}% da informação original foi descartada, mas a maior parte dos padrões presentes no conjunto de dados foi preservada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados redundantes e impacto\n",
    "print(\"\\nAo aplicar o PCA, eliminamos redundâncias causadas por variáveis altamente correlacionadas,\")\n",
    "print(\"o que ajuda a simplificar o problema, evitando que o modelo de clusterização seja influenciado por essas correlações desnecessárias.\")\n",
    "print(f\"A redução de variáveis facilita a clusterização, tornando o processo mais eficiente e ajudando a prevenir o overfitting.\")\n",
    "print(f\"No entanto, uma pequena quantidade de informação pode ser perdida ({100 - variancia_total:.2f}% da variância), o que é aceitável se a variância retida for alta o suficiente para capturar os principais padrões dos dados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impacto na clusterização\n",
    "print(\"\\nO impacto dessa transformação no processo de clusterização é que agora os clusters serão formados com base nas características mais importantes dos dados,\")\n",
    "print(\"aquelas que realmente explicam a variabilidade entre os pontos. Isso pode resultar em uma segmentação mais precisa e coesa,\")\n",
    "print(\"com menos influência de 'ruído' ou variáveis que não contribuem significativamente para a separação entre os grupos.\")\n",
    "print(f\"A margem fidedigna das informações retidas é de {variancia_total:.2f}%, o que é considerado alto e deve garantir uma boa representação dos dados originais.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METODO DE ELBOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a lista de número de clusters (por exemplo, de 2 a 12 clusters)\n",
    "num_clusters = list(range(2, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para armazenar os valores de inércia e silhouette score para cada número de clusters\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "centroids = []\n",
    "k_euclid = []\n",
    "dist = []\n",
    "soma_quadrado_intra_cluster = []\n",
    "soma_quadrados_entre_cluster = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a inércia e o silhouette score para cada número de clusters\n",
    "for k in num_clusters:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, init='k-means++')\n",
    "    kmeans.fit(pca)  # Aplicar o KMeans no conjunto de dados transformados pelo PCA\n",
    "    \n",
    "    # Adicionar inércia\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Só calcular o silhouette score se o número de clusters for maior que 1\n",
    "    if k > 1:\n",
    "        score = silhouette_score(pca, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "    \n",
    "    # Armazenar os centróides do cluster\n",
    "    centroids.append(kmeans.cluster_centers_)\n",
    "    \n",
    "    # Calcular as distâncias euclidianas para cada número de clusters\n",
    "    k_euclid = cdist(pca, kmeans.cluster_centers_, 'euclidean')\n",
    "    dist.append(np.min(k_euclid, axis=1))\n",
    "    \n",
    "    # Soma dos quadrados intra-cluster\n",
    "    soma_quadrado_intra_cluster.append(sum(dist[-1]**2))\n",
    "\n",
    "# Verificar resultados\n",
    "print(f\"Inércia: {inertia}\")\n",
    "print(f\"Silhouette Scores: {silhouette_scores}\")\n",
    "print(f\"Soma dos quadrados intra-cluster: {soma_quadrado_intra_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soma total dos quadrados\n",
    "soma_total = sum(pdist(pca)**2) / pca.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soma dos quadrados entre os clusters\n",
    "soma_quadrados_entre_cluster = [soma_total - intra for intra in soma_quadrado_intra_cluster]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora os comprimentos de num_clusters, inertia e silhouette_scores serão iguais\n",
    "#(f\"Tamanho de num_clusters: {len(num_clusters)}\")\n",
    "print(f\"Tamanho de inertia: {len(inertia)}\")\n",
    "print(f\"Tamanho de silhouette_scores: {len(silhouette_scores)}\")\n",
    "\n",
    "# Exibir as somas dos quadrados intra e entre clusters\n",
    "print(f\"Soma dos quadrados intra-cluster: {soma_quadrado_intra_cluster}\")\n",
    "print(f\"Soma dos quadrados entre-cluster: {soma_quadrados_entre_cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTADO ELBOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifique o tamanho de num_clusters e soma_quadrados_entre_cluster para garantir o mesmo comprimento\n",
    "num_clusters = num_clusters[:len(soma_quadrados_entre_cluster)]  # Ajustar para ter o mesmo comprimento\n",
    "\n",
    "# Plotar os dois gráficos lado a lado\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Gráfico 1: Variância Explicada\n",
    "ax1.plot(num_clusters, soma_quadrados_entre_cluster[:len(num_clusters)]/soma_total * 100, 'b*-')\n",
    "ax1.set_ylim((0, 100))\n",
    "ax1.grid(True)\n",
    "ax1.axhline(y=70, color='r', linestyle='--', label='70% Variância Explicada')\n",
    "ax1.axhline(y=80, color='g', linestyle='--', label='80% Variância Explicada')\n",
    "ax1.set_xlabel('Número de Clusters')\n",
    "ax1.set_ylabel('Percentual de Variância Explicada')\n",
    "ax1.set_title('Variância Explicada x Valor de K')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "# Gráfico 2: Silhouette Score\n",
    "sns.lineplot(x=num_clusters, y=silhouette_scores[:len(num_clusters)], color='darkorange', marker='o', ax=ax2)\n",
    "ax2.axhline(y=0.50, color='r', linestyle='--', label='Limite Inferior Aceitável (0.5)')\n",
    "ax2.axhline(y=0.70, color='g', linestyle='--', label='Limite Superior Aceitável (0.7)')\n",
    "ax2.set_xlabel('Número de Clusters', size=12)\n",
    "ax2.set_ylabel('Silhouette Score', size=12)\n",
    "ax2.set_title('Silhouette Score por Número de Clusters', size=20)\n",
    "ax2.legend(loc='best')\n",
    "\n",
    "# Exibir os gráficos\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop para calcular o silhouette score para diferentes números de clusters\n",
    "for k in num_clusters:\n",
    "    modelo = KMeans(n_clusters=k, random_state=42, init='k-means++')  # Ajustar o número de clusters\n",
    "    labels = modelo.fit_predict(pca)  # Fit e predict com o modelo\n",
    "    acuracia = silhouette_score(pca, labels, metric='euclidean')  # Calcular o silhouette score\n",
    "    silhouette_scores.append(acuracia)  # Armazenar o score\n",
    "    print(f\"Número de clusters: {k}, Acurácia (Silhouette Score): {acuracia}\")\n",
    "\n",
    "# Exibir o número de clusters com o melhor Silhouette Score\n",
    "best_num_clusters = num_clusters[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f\"Melhor número de clusters com base no Silhouette Score: {best_num_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir o melhor Silhouette Score e a inércia correspondente\n",
    "best_silhouette = max(silhouette_scores)\n",
    "best_clusters = num_clusters[silhouette_scores.index(best_silhouette)]\n",
    "print(f\"Melhor Silhouette Score: {best_silhouette}, Número de Clusters: {best_clusters}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELO CLUSTERIZAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando KMeans nos dados reduzidos pelo PCA\n",
    "kmeans = KMeans(n_clusters=best_num_clusters, random_state=42, init='k-means++')\n",
    "\n",
    "# Ajustando o KMeans com os dados transformados pelo PCA\n",
    "y_pred_kmeans = kmeans.fit_predict(pca)  # Gerando os rótulos dos clusters\n",
    "\n",
    "# Agora você pode acessar os labels dos clusters\n",
    "kmeans_labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculando as distâncias euclidianas dos pontos ao centroide do seu cluster\n",
    "distances = cdist(pca, kmeans.cluster_centers_, 'euclidean')\n",
    "# Tirando a menor distância (do centroide do cluster do ponto) para cada ponto\n",
    "min_distances = np.min(distances, axis=1)\n",
    "\n",
    "# Verificando as distâncias médias\n",
    "print(f'Distância média ao centróide: {np.mean(min_distances)}')\n",
    "print(f'Distância mínima ao centróide: {np.min(min_distances)}')\n",
    "print(f'Distância máxima ao centróide: {np.max(min_distances)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o silhouette score para KMeans\n",
    "if len(np.unique(y_pred_kmeans)) > 1:\n",
    "    silhouette_kmeans = silhouette_score(pca, y_pred_kmeans)\n",
    "    print(f'Silhouette Score para KMeans: {silhouette_kmeans}')\n",
    "else:\n",
    "    print(\"Silhouette Score não pode ser calculado, pois todos os pontos estão em um único cluster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calinski-Harabasz Index\n",
    "chi = calinski_harabasz_score(pca, y_pred_kmeans)\n",
    "print(f'Calinski-Harabasz Index: {chi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Davies-Bouldin Index\n",
    "dbi = davies_bouldin_score(pca, y_pred_kmeans)\n",
    "print(f'Davies-Bouldin Index: {dbi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisando Calinski-Harabasz Index\n",
    "if chi > 2000:\n",
    "    print(f\"Calinski-Harabasz Index ({chi}) é excelente! Clusters muito bem separados.\")\n",
    "elif 1000 <= chi <= 2000:\n",
    "    print(f\"Calinski-Harabasz Index ({chi}) é bom. Clusters bem separados, mas há espaço para melhorar.\")\n",
    "else:\n",
    "    print(f\"Calinski-Harabasz Index ({chi}) é baixo. Clusters não estão bem separados e precisam de ajuste.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise dos Resultados\n",
    "print(\"\\nAnalisando os Resultados:\")\n",
    "# Analisando Davies-Bouldin Index\n",
    "if dbi < 0.5:\n",
    "    print(f\"Davies-Bouldin Index ({dbi}) é excelente! Clusters muito bem separados e compactos.\")\n",
    "elif 0.5 <= dbi <= 1:\n",
    "    print(f\"Davies-Bouldin Index ({dbi}) é bom. Clusters bem definidos, mas pode haver um pouco de sobreposição.\")\n",
    "else:\n",
    "    print(f\"Davies-Bouldin Index ({dbi}) é alto. Pode haver muita sobreposição entre clusters ou clusters mal formados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Ajustar o tamanho da figura para exibir os três gráficos lado a lado\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Gráfico 2D: Visualizando os clusters e os centróides no plano 2D usando PCA\n",
    "ax1 = fig.add_subplot(131)\n",
    "sc1 = ax1.scatter(pca[:, 0], pca[:, 1], c=kmeans.labels_, cmap='viridis')  # Colorir pelos clusters\n",
    "ax1.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=200, marker='X', label='Centroides')  # Adicionar os centróides\n",
    "ax1.set_title(\"Clusters com base no PCA (2D) - Com Centrôides\")\n",
    "ax1.set_xlabel(\"Componente Principal 1\")\n",
    "ax1.set_ylabel(\"Componente Principal 2\")\n",
    "fig.colorbar(sc1, ax=ax1, label=\"Clusters\")\n",
    "\n",
    "# Refazendo o PCA com 3 componentes para visualização 3D\n",
    "pca_3d = PCA(n_components=3)\n",
    "pca_3d_data = pca_3d.fit_transform(df)  # Substitua df pelos seus dados\n",
    "\n",
    "# Gerando o gráfico 3D\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "sc2 = ax2.scatter(pca_3d_data[:, 0], pca_3d_data[:, 1], pca_3d_data[:, 2], c=kmeans.labels_, cmap='viridis')  # Colorir pela coluna ISA\n",
    "ax2.set_xlabel('Componente Principal 1')\n",
    "ax2.set_ylabel('Componente Principal 2')\n",
    "ax2.set_zlabel('Componente Principal 3')\n",
    "ax2.set_title('Visualização 3D dos Clusters - Colorido por ISA')\n",
    "fig.colorbar(sc2, ax=ax2, label=\"Clusters\")\n",
    "\n",
    "# Gráfico de Silhouette\n",
    "ax3 = fig.add_subplot(133)\n",
    "y_lower = 10\n",
    "sample_silhouette_values = silhouette_samples(pca, kmeans.labels_)\n",
    "\n",
    "# Configurar limites do gráfico de Silhouette\n",
    "ax3.set_xlim([-0.1, 1])\n",
    "ax3.set_ylim([0, len(pca) + (kmeans.n_clusters + 1) * 10])\n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[kmeans.labels_ == i]\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / kmeans.n_clusters)\n",
    "    ax3.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label dos clusters no meio\n",
    "    ax3.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    y_lower = y_upper + 10  # 10 para espaço entre clusters\n",
    "\n",
    "# Linha vertical do valor médio do Silhouette Score\n",
    "silhouette_avg = sample_silhouette_values.mean()\n",
    "ax3.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax3.set_title(\"Gráfico de Silhouette para os Clusters\")\n",
    "ax3.set_xlabel(\"Valores do coeficiente de silhouette\")\n",
    "ax3.set_ylabel(\"Cluster\")\n",
    "\n",
    "# Ajuste do layout para evitar sobreposição de elementos\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final do Script')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
